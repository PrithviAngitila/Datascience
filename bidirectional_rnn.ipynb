{"nbformat_minor": 2, "cells": [{"source": "### Optical character recognition using RNNs", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "!pip install --upgrade numpy\n!pip install --upgrade tensorflow", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Collecting numpy\n  Using cached https://files.pythonhosted.org/packages/85/51/ba4564ded90e093dbb6adfc3e21f99ae953d9ad56477e1b0d4a93bacf7d3/numpy-1.15.0-cp27-cp27mu-manylinux1_x86_64.whl\nInstalling collected packages: numpy\n  Found existing installation: numpy 1.14.5\n    Uninstalling numpy-1.14.5:\n      Successfully uninstalled numpy-1.14.5\nSuccessfully installed numpy-1.15.0\n\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\nRequirement already up-to-date: tensorflow in /home/nbuser/anaconda2_20/lib/python2.7/site-packages\nRequirement already up-to-date: enum34>=1.1.6 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: mock>=2.0.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: protobuf>=3.6.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: absl-py>=0.1.6 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: grpcio>=1.8.6 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: six>=1.10.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: tensorboard<1.11.0,>=1.10.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: termcolor>=1.1.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: gast>=0.2.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: wheel in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: setuptools<=39.1.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: backports.weakref>=1.0rc1 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nRequirement already up-to-date: astor>=0.6.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorflow)\nCollecting numpy<=1.14.5,>=1.13.3 (from tensorflow)\n  Using cached https://files.pythonhosted.org/packages/6a/a9/c01a2d5f7b045f508c8cefef3b079fe8c413d05498ca0ae877cffa230564/numpy-1.14.5-cp27-cp27mu-manylinux1_x86_64.whl\nRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already up-to-date: pbr>=0.11 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from mock>=2.0.0->tensorflow)\nRequirement already up-to-date: futures>=2.2.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from grpcio>=1.8.6->tensorflow)\nRequirement already up-to-date: werkzeug>=0.11.10 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow)\nRequirement already up-to-date: markdown>=2.6.8 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow)\nInstalling collected packages: numpy\n  Found existing installation: numpy 1.15.0\n    Uninstalling numpy-1.15.0:\n      Successfully uninstalled numpy-1.15.0\nSuccessfully installed numpy-1.14.5\n\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"}], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function", "outputs": [], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "import os\nimport gzip\nimport csv", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "import numpy as np\nimport tensorflow as tf", "outputs": [{"output_type": "stream", "name": "stderr", "text": "/home/nbuser/anaconda2_20/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"}], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt", "outputs": [], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "from six.moves import urllib", "outputs": [], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "print(np.__version__)\nprint(tf.__version__)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1.14.5\n1.10.0\n"}], "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "URL_PATH = 'http://ai.stanford.edu/~btaskar/ocr/letter.data.gz'\nDOWNLOADED_FILENAME = 'letter.data.gz'\n\ndef download_data():\n    if not os.path.exists(DOWNLOADED_FILENAME):\n        filename, _ = urllib.request.urlretrieve(URL_PATH, DOWNLOADED_FILENAME)\n    \n    print('Found and verified file from this path: ', URL_PATH)\n    print('Downloaded file: ', DOWNLOADED_FILENAME)", "outputs": [], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "download_data()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Found and verified file from this path:  http://ai.stanford.edu/~btaskar/ocr/letter.data.gz\nDownloaded file:  letter.data.gz\n"}], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "def read_lines():\n    with gzip.open(DOWNLOADED_FILENAME, 'rt') as f:\n        reader = csv.reader(f, delimiter='\\t')\n        lines = list(reader)\n\n        return lines", "outputs": [], "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "lines = read_lines()", "outputs": [], "metadata": {}}, {"source": "### Format of every line\n\n* id\n* letter\n* next_id\n* word_id\n* position\n* fold\n* 16x8 columns of pixel values", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "lines[0][:8]", "outputs": [{"execution_count": 12, "output_type": "execute_result", "data": {"text/plain": "['1', 'o', '2', '1', '1', '0', '0', '0']"}, "metadata": {}}], "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "len(lines)", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {"text/plain": "52152"}, "metadata": {}}], "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "def get_features_labels(lines):\n    lines = sorted(lines, key=lambda x: int(x[0]))\n    data, target = [], []\n    \n    next_id = -1\n    \n    word = []\n    word_pixels = []\n\n    for line in lines:\n        next_id = int(line[2]) # The index for the next_id column\n\n        pixels = np.array([int(x) for x in line[6:134]])\n        pixels = pixels.reshape((16, 8))\n        \n        word_pixels.append(pixels)\n        word.append(line[1])\n        \n        if next_id == -1:\n            data.append(word_pixels)\n            target.append(word)\n\n            word = []\n            word_pixels = []\n\n\n    return data, target", "outputs": [], "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "data, target = get_features_labels(lines)", "outputs": [], "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "def pad_features_labels(data, target):    \n    max_length = max(len(x) for x in target)\n    padding = np.zeros((16, 8))\n\n    data = [x + ([padding] * (max_length - len(x))) for x in data]\n    target = [x + ([''] * (max_length - len(x))) for x in target]\n    \n    return np.array(data), np.array(target)", "outputs": [], "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "padded_data, padded_target = pad_features_labels(data, target)", "outputs": [], "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "padded_target[:10]", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {"text/plain": "array([['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', ''],\n       ['o', 'm', 'm', 'a', 'n', 'd', 'i', 'n', 'g', '', '', '', '', '']],\n      dtype='|S1')"}, "metadata": {}}], "metadata": {}}, {"source": "#### The length of each sequence\n\nWe've padded all words so that their lengths are all equal to the length of the longest word", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "sequence_length = len(padded_target[0])", "outputs": [], "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "sequence_length", "outputs": [{"execution_count": 20, "output_type": "execute_result", "data": {"text/plain": "14"}, "metadata": {}}], "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "padded_data.shape", "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "(6877, 14, 16, 8)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 22, "cell_type": "code", "source": "padded_data.shape[:2] + (-1,)", "outputs": [{"execution_count": 22, "output_type": "execute_result", "data": {"text/plain": "(6877, 14, -1)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 23, "cell_type": "code", "source": "reshaped_data = padded_data.reshape(padded_data.shape[:2] + (-1,))", "outputs": [], "metadata": {}}, {"execution_count": 24, "cell_type": "code", "source": "reshaped_data.shape", "outputs": [{"execution_count": 24, "output_type": "execute_result", "data": {"text/plain": "(6877, 14, 128)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": "padded_target.shape", "outputs": [{"execution_count": 25, "output_type": "execute_result", "data": {"text/plain": "(6877, 14)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": "padded_target.shape + (26,)", "outputs": [{"execution_count": 26, "output_type": "execute_result", "data": {"text/plain": "(6877, 14, 26)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": "one_hot_target = np.zeros(padded_target.shape + (26,))", "outputs": [], "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": "for index, letter in np.ndenumerate(padded_target):\n    if letter:\n        one_hot_target[index][ord(letter) - ord('a')] = 1", "outputs": [], "metadata": {}}, {"source": "#### One-hot representation of the letter 'o'\n\n* The letter 'o' represented by a 1 at the 14th index \n* Index positions start at 0", "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": "one_hot_target[0][0]", "outputs": [{"execution_count": 29, "output_type": "execute_result", "data": {"text/plain": "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0.])"}, "metadata": {}}], "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": "shuffled_indices = np.random.permutation(len(reshaped_data))\n\nshuffled_data = reshaped_data[shuffled_indices]\nshuffled_target = one_hot_target[shuffled_indices]", "outputs": [], "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": "split = int(0.66 * len(shuffled_data))\n\ntrain_data = shuffled_data[:split]\ntrain_target = shuffled_target[:split]\n\ntest_data = shuffled_data[split:]\ntest_target = shuffled_target[split:]", "outputs": [], "metadata": {}}, {"execution_count": 32, "cell_type": "code", "source": "train_data.shape", "outputs": [{"execution_count": 32, "output_type": "execute_result", "data": {"text/plain": "(4538, 14, 128)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": "_, num_steps, num_inputs = train_data.shape", "outputs": [], "metadata": {}}, {"execution_count": 34, "cell_type": "code", "source": "train_target.shape", "outputs": [{"execution_count": 34, "output_type": "execute_result", "data": {"text/plain": "(4538, 14, 26)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 68, "cell_type": "code", "source": "#keras implementation for BI-RNN\n\ntrain_target_reshaped=train_target.reshape(train_target.shape[0],-1)\ntrain_target_reshaped.shape", "outputs": [{"execution_count": 68, "output_type": "execute_result", "data": {"text/plain": "(4538, 364)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 69, "cell_type": "code", "source": "test_target_reshaped=test_target.reshape(test_target.shape[0],-1)\ntest_target_reshaped.shape", "outputs": [{"execution_count": 69, "output_type": "execute_result", "data": {"text/plain": "(2339, 364)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 76, "cell_type": "code", "source": "!pip install keras", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Collecting keras\n  Downloading https://files.pythonhosted.org/packages/34/7d/b1dedde8af99bd82f20ed7e9697aac0597de3049b1f786aa2aac3b9bd4da/Keras-2.2.2-py2.py3-none-any.whl (299kB)\n\u001b[K    100% |\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 307kB 1.6MB/s ta 0:00:01\n\u001b[?25hRequirement already satisfied: pyyaml in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from keras)\nCollecting keras-preprocessing==1.0.2 (from keras)\n  Downloading https://files.pythonhosted.org/packages/71/26/1e778ebd737032749824d5cba7dbd3b0cf9234b87ab5ec79f5f0403ca7e9/Keras_Preprocessing-1.0.2-py2.py3-none-any.whl\nRequirement already satisfied: scipy>=0.14 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from keras)\nRequirement already satisfied: six>=1.9.0 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from keras)\nRequirement already satisfied: numpy>=1.9.1 in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from keras)\nCollecting keras-applications==1.0.4 (from keras)\n  Downloading https://files.pythonhosted.org/packages/54/90/8f327deaa37a71caddb59b7b4aaa9d4b3e90c0e76f8c2d1572005278ddc5/Keras_Applications-1.0.4-py2.py3-none-any.whl (43kB)\n\u001b[K    100% |\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088\u00e2\u0096\u0088| 51kB 8.4MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: h5py in /home/nbuser/anaconda2_20/lib/python2.7/site-packages (from keras)\nInstalling collected packages: keras-preprocessing, keras-applications, keras\nSuccessfully installed keras-2.2.2 keras-applications-1.0.4 keras-preprocessing-1.0.2\n\u001b[33mYou are using pip version 9.0.3, however version 18.0 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"}], "metadata": {}}, {"execution_count": 106, "cell_type": "code", "source": "from keras.layers import GRU,LSTM,Dense,Dropout,Bidirectional,Input\nfrom keras import optimizers \nfrom  keras.preprocessing  import sequence\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nmodel=Sequential()\n#model.add(Input(input_shape=(14,128))\nx=GRU(input_shape=(14,128),units=150, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.3, recurrent_dropout=0.4, implementation=1, return_sequences=True, return_state=False, go_backwards=False, stateful=False, unroll=True)\nlstm=LSTM(input_shape=(14,128),units=150,return_sequences=True)\nmodel.add(Bidirectional(lstm, merge_mode='concat', weights=None))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(26,activation='sigmoid'))\n\n#forward pass\n", "outputs": [], "metadata": {}}, {"execution_count": 107, "cell_type": "code", "source": "model.compile(loss='binary_crossentropy',  \n            optimizer='RMSProp',              \n            metrics=['accuracy'])", "outputs": [], "metadata": {}}, {"execution_count": 109, "cell_type": "code", "source": "BATCH_SIZE = 24\nEPOCHS = 8\ncbk_early_stopping = EarlyStopping(monitor='val_acc', patience=2, mode='max')\nmodel.fit(train_data, train_target, BATCH_SIZE, epochs=EPOCHS, \n             validation_data=(test_data, test_target),\n            callbacks=[cbk_early_stopping] )", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Train on 4538 samples, validate on 2339 samples\nEpoch 1/8\n4538/4538 [==============================] - 60s 13ms/step - loss: 0.0135 - acc: 0.9958 - val_loss: 0.0132 - val_acc: 0.9960\nEpoch 2/8\n4538/4538 [==============================] - 67s 15ms/step - loss: 0.0104 - acc: 0.9969 - val_loss: 0.0123 - val_acc: 0.9963\nEpoch 3/8\n4538/4538 [==============================] - 63s 14ms/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.0090 - val_acc: 0.9972\nEpoch 4/8\n4538/4538 [==============================] - 65s 14ms/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0074 - val_acc: 0.9977\nEpoch 5/8\n4538/4538 [==============================] - 63s 14ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0066 - val_acc: 0.9979\nEpoch 6/8\n4538/4538 [==============================] - 66s 15ms/step - loss: 0.0037 - acc: 0.9990 - val_loss: 0.0059 - val_acc: 0.9981\nEpoch 7/8\n4538/4538 [==============================] - 61s 14ms/step - loss: 0.0029 - acc: 0.9993 - val_loss: 0.0054 - val_acc: 0.9983\nEpoch 8/8\n4538/4538 [==============================] - 62s 14ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.0047 - val_acc: 0.9985\n"}, {"execution_count": 109, "output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7faa9eedea90>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 110, "cell_type": "code", "source": "#tensorflow starts here", "outputs": [], "metadata": {}}, {"execution_count": 111, "cell_type": "code", "source": "num_classes = train_target.shape[2]", "outputs": [], "metadata": {}}, {"execution_count": 112, "cell_type": "code", "source": "tf.reset_default_graph()", "outputs": [], "metadata": {}}, {"execution_count": 113, "cell_type": "code", "source": "X = tf.placeholder(tf.float64, [None, num_steps, num_inputs])\n\ny = tf.placeholder(tf.float64, [None, num_steps, num_classes])", "outputs": [], "metadata": {}}, {"source": "#### Sequence length calculation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 114, "cell_type": "code", "source": "used = tf.sign(tf.reduce_max(tf.abs(X), reduction_indices=2))\n\nlength = tf.reduce_sum(used, reduction_indices=1)\nsequence_length = tf.cast(length, tf.int64)", "outputs": [], "metadata": {}}, {"execution_count": 115, "cell_type": "code", "source": "sequence_length", "outputs": [{"execution_count": 115, "output_type": "execute_result", "data": {"text/plain": "<tf.Tensor 'Cast:0' shape=(?,) dtype=int64>"}, "metadata": {}}], "metadata": {}}, {"source": "#### RNN for training and prediction", "cell_type": "markdown", "metadata": {}}, {"execution_count": 116, "cell_type": "code", "source": "num_neurons = 300", "outputs": [], "metadata": {}}, {"source": "#### Forward RNN to feed in each word in the right order\n\nMake sure you specify a scope for each RNN so you can initialize multiple RNNs in the same graph (the default scope is *'rnn'* which will clash across the two RNNs we set up)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 117, "cell_type": "code", "source": "forward, _ = tf.nn.dynamic_rnn(tf.nn.rnn_cell.GRUCell(num_neurons), X,\n                               dtype=tf.float64, sequence_length=sequence_length,\n                               scope='rnn-forward')", "outputs": [], "metadata": {}}, {"source": "#### Reverse the characters in each word and feed in to another forward RNN\n\n* Reverse the 1st dimension i.e the characters\n* Note that only the actual sequence length of the characters are reversed, the padding is not reversed", "cell_type": "markdown", "metadata": {}}, {"execution_count": 118, "cell_type": "code", "source": "X_reversed = tf.reverse_sequence(X, sequence_length, seq_dim=1)\n\nbackward, _ = tf.nn.dynamic_rnn(tf.nn.rnn_cell.GRUCell(num_neurons), X_reversed,\n                               dtype=tf.float64, sequence_length=sequence_length,\n                               scope='rnn-backward')", "outputs": [], "metadata": {}}, {"source": "#### Get output back in the original order", "cell_type": "markdown", "metadata": {}}, {"execution_count": 119, "cell_type": "code", "source": "backward = tf.reverse_sequence(backward, sequence_length, seq_dim=1)", "outputs": [], "metadata": {}}, {"execution_count": 120, "cell_type": "code", "source": "backward, forward", "outputs": [{"execution_count": 120, "output_type": "execute_result", "data": {"text/plain": "(<tf.Tensor 'ReverseSequence_1:0' shape=(?, 14, 300) dtype=float64>,\n <tf.Tensor 'rnn-forward/transpose_1:0' shape=(?, 14, 300) dtype=float64>)"}, "metadata": {}}], "metadata": {}}, {"execution_count": 121, "cell_type": "code", "source": "output = tf.concat([forward, backward], axis=2)", "outputs": [], "metadata": {}}, {"execution_count": 122, "cell_type": "code", "source": "output.shape", "outputs": [{"execution_count": 122, "output_type": "execute_result", "data": {"text/plain": "TensorShape([Dimension(None), Dimension(14), Dimension(600)])"}, "metadata": {}}], "metadata": {}}, {"source": "#### Shared softmax layer", "cell_type": "markdown", "metadata": {}}, {"execution_count": 123, "cell_type": "code", "source": "weight = tf.Variable(tf.truncated_normal([num_neurons * 2, num_classes], stddev=0.01, dtype=tf.float64))", "outputs": [], "metadata": {}}, {"execution_count": 124, "cell_type": "code", "source": "bias = tf.Variable(tf.constant(0.1, shape=[num_classes], dtype=tf.float64))", "outputs": [], "metadata": {}}, {"execution_count": 125, "cell_type": "code", "source": "flattened_output = tf.reshape(output, [-1, num_neurons * 2])", "outputs": [], "metadata": {}}, {"execution_count": 126, "cell_type": "code", "source": "flattened_output", "outputs": [{"execution_count": 126, "output_type": "execute_result", "data": {"text/plain": "<tf.Tensor 'Reshape:0' shape=(?, 600) dtype=float64>"}, "metadata": {}}], "metadata": {}}, {"execution_count": 127, "cell_type": "code", "source": "logits = tf.matmul(flattened_output, weight) + bias", "outputs": [], "metadata": {}}, {"execution_count": 128, "cell_type": "code", "source": "logits_reshaped = tf.reshape(logits, [-1, num_steps, num_classes])", "outputs": [], "metadata": {}}, {"source": "#### Cost calculation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 129, "cell_type": "code", "source": "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)", "outputs": [], "metadata": {}}, {"execution_count": 130, "cell_type": "code", "source": "loss = tf.reduce_mean(cross_entropy)", "outputs": [], "metadata": {}}, {"source": "#### Error calculation", "cell_type": "markdown", "metadata": {}}, {"execution_count": 131, "cell_type": "code", "source": "mistakes = tf.not_equal(\n            tf.argmax(y, 2), tf.argmax(logits_reshaped, 2))\nmistakes = tf.cast(mistakes, tf.float64)\n\nmask = tf.sign(tf.reduce_max(tf.abs(y), reduction_indices=2))\nmistakes *= mask", "outputs": [], "metadata": {}}, {"execution_count": 132, "cell_type": "code", "source": "mistakes = tf.reduce_sum(mistakes, reduction_indices=1)\nmistakes /= tf.cast(sequence_length, tf.float64)", "outputs": [], "metadata": {}}, {"execution_count": 133, "cell_type": "code", "source": "error = tf.reduce_mean(mistakes)", "outputs": [], "metadata": {}}, {"source": "#### Optimizer", "cell_type": "markdown", "metadata": {}}, {"execution_count": 134, "cell_type": "code", "source": "optimizer = tf.train.RMSPropOptimizer(0.002)", "outputs": [], "metadata": {}}, {"execution_count": 135, "cell_type": "code", "source": "gradient = optimizer.compute_gradients(loss)", "outputs": [], "metadata": {}}, {"execution_count": 136, "cell_type": "code", "source": "optimize = optimizer.apply_gradients(gradient)", "outputs": [], "metadata": {}}, {"execution_count": 137, "cell_type": "code", "source": "def batched(data, target, batch_size):\n    epoch = 0\n    offset = 0\n    while True:\n        old_offset = offset\n        offset = (offset + batch_size) % (target.shape[0] - batch_size)\n\n        # Offset wrapped around to the beginning so new epoch\n        if offset < old_offset:\n            # New epoch, need to shuffle data\n            shuffled_indices = np.random.permutation(len(data))\n            \n            data = data[shuffled_indices]\n            target = target[shuffled_indices]\n\n            epoch += 1\n\n        batch_data = data[offset:(offset + batch_size), :]\n        \n        batch_target = target[offset:(offset + batch_size), :]\n\n        yield batch_data, batch_target, epoch", "outputs": [], "metadata": {}}, {"execution_count": 138, "cell_type": "code", "source": "batch_size = 10\nbatches = batched(train_data, train_target, batch_size)", "outputs": [], "metadata": {}}, {"execution_count": 139, "cell_type": "code", "source": "epochs = 5", "outputs": [], "metadata": {}}, {"execution_count": 140, "cell_type": "code", "source": "with tf.Session() as sess:\n    \n    sess.run(tf.global_variables_initializer())\n\n    for index, batch in enumerate(batches):\n        batch_data = batch[0]\n        batch_target = batch[1]\n    \n        epoch = batch[2]\n\n        if epoch >= epochs:\n            break\n        \n        feed = {X: batch_data, y: batch_target}\n        train_error, _ = sess.run([error, optimize], feed)\n        \n        print('{}: {:3.6f}%'.format(index + 1, 100 * train_error))\n\n    test_feed = {X: test_data, y: test_target}\n    test_error, _ = sess.run([error, optimize], test_feed)\n    \n    print('Test error: {:3.6f}%'.format(100 * test_error))", "outputs": [{"output_type": "stream", "name": "stdout", "text": "1: 99.090909%\n2: 96.750000%\n3: 100.000000%\n4: 95.238095%\n5: 94.333333%\n6: 92.564103%\n7: 93.412698%\n8: 100.000000%\n9: 98.888889%\n10: 99.000000%\n11: 95.071429%\n12: 95.202020%\n13: 97.181818%\n14: 94.841270%\n15: 94.416667%\n16: 90.555556%\n17: 97.638889%\n18: 94.432789%\n19: 94.404762%\n20: 95.691087%\n21: 94.282246%\n22: 91.158009%\n23: 96.135531%\n24: 93.960567%\n25: 95.777778%\n26: 86.861111%\n27: 90.190476%\n28: 92.397436%\n29: 96.904762%\n30: 89.713564%\n31: 91.250000%\n32: 92.699634%\n33: 86.327839%\n34: 95.638889%\n35: 93.273810%\n36: 91.468254%\n37: 92.460317%\n38: 91.515873%\n39: 88.154762%\n40: 90.445055%\n41: 91.750000%\n42: 84.360750%\n43: 83.852092%\n44: 87.904762%\n45: 82.619658%\n46: 89.671856%\n47: 96.497253%\n48: 86.472222%\n49: 96.666667%\n50: 84.547619%\n51: 89.293651%\n52: 93.397436%\n53: 86.435786%\n54: 90.354645%\n55: 88.000000%\n56: 86.100427%\n57: 88.614358%\n58: 92.015873%\n59: 81.021062%\n60: 92.643468%\n61: 90.818071%\n62: 92.365690%\n63: 87.460317%\n64: 84.255189%\n65: 88.975469%\n66: 94.350427%\n67: 86.912587%\n68: 90.134810%\n69: 94.914530%\n70: 89.683761%\n71: 89.289322%\n72: 91.869658%\n73: 86.810745%\n74: 90.602453%\n75: 90.995671%\n76: 90.694444%\n77: 87.687729%\n78: 80.210317%\n79: 89.218864%\n80: 78.212121%\n81: 82.710317%\n82: 87.603175%\n83: 90.932540%\n84: 91.301476%\n85: 94.024420%\n86: 92.404040%\n87: 87.769120%\n88: 89.988095%\n89: 80.863636%\n90: 88.194444%\n91: 84.416667%\n92: 90.237374%\n93: 87.265512%\n94: 83.292208%\n95: 87.445055%\n96: 88.181818%\n97: 78.458874%\n98: 87.739316%\n99: 90.241841%\n100: 90.277778%\n101: 87.174603%\n102: 70.429487%\n103: 86.436203%\n104: 83.015873%\n105: 81.696970%\n106: 78.998085%\n107: 82.985348%\n108: 78.587302%\n109: 81.634921%\n110: 77.858974%\n111: 82.805556%\n112: 80.734127%\n113: 92.000000%\n114: 80.883838%\n115: 84.270480%\n116: 71.442308%\n117: 70.067460%\n118: 79.018926%\n119: 77.990981%\n120: 74.317460%\n121: 80.696886%\n122: 86.540904%\n123: 76.827839%\n124: 75.606061%\n125: 86.851038%\n126: 79.642857%\n127: 77.339133%\n128: 75.829976%\n129: 70.597070%\n130: 76.117521%\n131: 70.888889%\n132: 76.111860%\n133: 68.383838%\n134: 78.539322%\n135: 68.844156%\n136: 68.357587%\n137: 69.880952%\n138: 69.624847%\n139: 71.406926%\n140: 70.365079%\n141: 66.562771%\n142: 65.391331%\n143: 66.949023%\n144: 75.949495%\n145: 65.181319%\n146: 60.436508%\n147: 72.895382%\n148: 77.022727%\n149: 61.887141%\n150: 62.348429%\n151: 57.603175%\n152: 61.253968%\n153: 64.821429%\n154: 61.453824%\n155: 66.456876%\n156: 53.063187%\n157: 58.034188%\n158: 61.222222%\n159: 71.843712%\n160: 61.478355%\n161: 61.191919%\n162: 62.575397%\n163: 56.546898%\n164: 45.194444%\n165: 55.458874%\n166: 45.416667%\n167: 42.833333%\n168: 51.869048%\n169: 62.689255%\n170: 42.175741%\n171: 38.407426%\n172: 54.952076%\n173: 46.507937%\n174: 38.333333%\n175: 37.587302%\n176: 38.874820%\n177: 45.694750%\n178: 45.280303%\n179: 45.384615%\n180: 39.658730%\n181: 34.488095%\n182: 41.833333%\n183: 49.978355%\n184: 29.679487%\n185: 33.676601%\n186: 53.918110%\n187: 35.402930%\n188: 36.111111%\n189: 33.079365%\n190: 26.969697%\n191: 36.745726%\n192: 37.554113%\n193: 39.985570%\n194: 30.844322%\n195: 44.346154%\n196: 21.703907%\n197: 16.626984%\n198: 30.489649%\n199: 31.933983%\n200: 35.761905%\n201: 42.277778%\n202: 39.206349%\n203: 35.922633%\n204: 26.073232%\n205: 19.206349%\n206: 28.888889%\n207: 40.239538%\n208: 29.718254%\n209: 21.228938%\n210: 24.079365%\n211: 41.949689%\n212: 18.174603%\n213: 23.990232%\n214: 38.535714%\n215: 35.744811%\n216: 22.128788%\n217: 21.017316%\n218: 31.025641%\n219: 15.492979%\n220: 20.333333%\n221: 31.380952%\n222: 17.944444%\n223: 18.841270%\n224: 16.888889%\n225: 27.309524%\n226: 18.143939%\n227: 26.821429%\n228: 26.430098%\n229: 30.021284%\n230: 21.538462%\n231: 25.222222%\n232: 16.145604%\n233: 27.724359%\n234: 42.790765%\n235: 19.384921%\n236: 19.047619%\n237: 12.794012%\n238: 24.698413%\n239: 18.194444%\n240: 33.168498%\n241: 44.523810%\n242: 19.777778%\n243: 20.444444%\n244: 32.078866%\n245: 16.902320%\n246: 9.857143%\n247: 28.769841%\n248: 43.968254%\n249: 23.738095%\n250: 18.095238%\n251: 22.982295%\n252: 12.666667%\n253: 21.373016%\n254: 15.411477%\n255: 15.222222%\n256: 14.931319%\n257: 14.384921%\n258: 22.777778%\n259: 15.803419%\n260: 17.564574%\n261: 25.870130%\n262: 8.562271%\n263: 17.131313%\n264: 15.865801%\n265: 23.666667%\n266: 18.270202%\n267: 26.531136%\n268: 20.155067%\n269: 26.599512%\n270: 15.100733%\n271: 20.641026%\n272: 22.829670%\n273: 11.064935%\n274: 8.151515%\n275: 26.666667%\n276: 27.678571%\n277: 26.194444%\n278: 11.499389%\n279: 11.166278%\n280: 10.888889%\n281: 13.289683%\n282: 19.917860%\n283: 22.460317%\n284: 15.130952%\n285: 7.833333%\n286: 16.857143%\n287: 19.166667%\n288: 20.857143%\n289: 26.775613%\n290: 12.631313%\n291: 17.190476%\n292: 11.111111%\n293: 6.793651%\n294: 10.810606%\n295: 18.650794%\n296: 21.454185%\n297: 14.436869%\n298: 16.695527%\n299: 21.527778%\n300: 15.063492%\n301: 26.952381%\n302: 10.095238%\n303: 16.180708%\n304: 9.371795%\n305: 12.279915%\n306: 16.788462%\n307: 20.531136%\n308: 23.269231%\n309: 11.017871%\n310: 16.596875%\n311: 7.492424%\n312: 15.678571%\n313: 18.984848%\n314: 11.763792%\n315: 7.236264%\n316: 20.121795%\n317: 8.761905%\n318: 13.908730%\n319: 19.690476%\n320: 7.000000%\n321: 13.857143%\n322: 16.540404%\n323: 21.249084%\n324: 11.138889%\n325: 20.707071%\n326: 12.857143%\n327: 21.464286%\n328: 6.617827%\n329: 9.777778%\n330: 10.888889%\n331: 14.666667%\n332: 13.472222%\n333: 8.269231%\n334: 14.193223%\n335: 12.746032%\n336: 16.742424%\n337: 19.837662%\n338: 5.396825%\n339: 15.595238%\n340: 19.750000%\n341: 12.539683%\n342: 14.224359%\n343: 12.747835%\n344: 20.293040%\n345: 11.380952%\n346: 10.444444%\n347: 9.976190%\n348: 6.039683%\n349: 3.290043%\n350: 18.258242%\n351: 9.681319%\n352: 15.968254%\n353: 5.333333%\n354: 9.583333%\n355: 8.277778%\n356: 16.529915%\n357: 10.333333%\n358: 13.984127%\n359: 25.347985%\n360: 15.583333%\n361: 7.222222%\n362: 15.167749%\n363: 8.809524%\n364: 17.818182%\n365: 2.873377%\n366: 12.896825%\n367: 11.666667%\n368: 5.027778%\n369: 10.649573%\n370: 3.777778%\n371: 7.861111%\n372: 18.174603%\n373: 7.307692%\n374: 7.555556%\n375: 23.216783%\n376: 12.454545%\n377: 18.246753%\n378: 12.222222%\n379: 4.583333%\n380: 8.293651%\n381: 2.000000%\n382: 19.203213%\n383: 2.142857%\n384: 14.777778%\n385: 21.125541%\n386: 9.345238%\n387: 17.523310%\n388: 4.333333%\n389: 9.166667%\n390: 21.205128%\n391: 25.012266%\n392: 10.274170%\n393: 12.222222%\n394: 7.619048%\n395: 4.825397%\n396: 8.809524%\n397: 0.000000%\n398: 8.663004%\n399: 15.581530%\n400: 10.987485%\n401: 10.606061%\n402: 10.952381%\n403: 11.666667%\n404: 8.650794%\n405: 2.337662%\n406: 9.685897%\n407: 14.523810%\n408: 20.416667%\n409: 6.245421%\n410: 10.138889%\n411: 13.076923%\n412: 6.214286%\n413: 7.587302%\n414: 16.666667%\n415: 0.000000%\n416: 7.750000%\n417: 20.158730%\n418: 7.301587%\n419: 10.416667%\n420: 1.428571%\n421: 13.333333%\n422: 11.102564%\n423: 4.539683%\n424: 9.500000%\n425: 10.865801%\n426: 5.519231%\n427: 3.650794%\n428: 5.352564%\n429: 5.352564%\n430: 2.727273%\n431: 5.353535%\n432: 8.575397%\n433: 4.761905%\n434: 5.595238%\n435: 19.817460%\n436: 17.838384%\n437: 15.277778%\n438: 6.111111%\n439: 8.333333%\n440: 8.472222%\n441: 8.961039%\n442: 17.567460%\n443: 15.297619%\n444: 11.666667%\n445: 20.333333%\n446: 14.541681%\n447: 15.923077%\n448: 19.500000%\n449: 5.428571%\n450: 12.121212%\n451: 17.287157%\n452: 5.000000%\n453: 5.333333%\n454: 8.150183%\n455: 0.000000%\n456: 8.888889%\n457: 2.678571%\n458: 8.472222%\n459: 4.633700%\n460: 9.877345%\n461: 2.019231%\n462: 1.000000%\n463: 8.060606%\n464: 0.000000%\n465: 1.483516%\n466: 2.000000%\n467: 6.666667%\n468: 8.611111%\n469: 5.333333%\n470: 11.241758%\n471: 5.714286%\n472: 5.714286%\n473: 2.000000%\n474: 1.880342%\n475: 6.666667%\n476: 4.935897%\n477: 1.666667%\n478: 2.777778%\n479: 7.666667%\n480: 4.000000%\n481: 12.927961%\n482: 6.300366%\n483: 10.317460%\n484: 5.000000%\n485: 11.250000%\n486: 12.222222%\n487: 9.727273%\n488: 7.222222%\n489: 9.944444%\n490: 2.678571%\n491: 2.222222%\n492: 10.444444%\n493: 1.111111%\n494: 7.142857%\n495: 3.111111%\n496: 1.111111%\n497: 1.483516%\n498: 5.476190%\n499: 2.000000%\n500: 3.846154%\n501: 5.000000%\n502: 7.571429%\n503: 5.357864%\n504: 7.474747%\n505: 1.000000%\n506: 8.782051%\n507: 0.000000%\n508: 4.000000%\n509: 5.000000%\n510: 11.333333%\n511: 17.159091%\n512: 0.769231%\n513: 4.583333%\n514: 4.833333%\n515: 8.666667%\n516: 5.555556%\n517: 1.250000%\n518: 7.777778%\n519: 2.159091%\n520: 13.333333%\n521: 8.194444%\n522: 0.000000%\n523: 9.029304%\n524: 3.333333%\n525: 1.111111%\n526: 3.111111%\n527: 3.944444%\n528: 3.538462%\n"}, {"output_type": "stream", "name": "stdout", "text": "529: 7.857143%\n530: 0.000000%\n531: 10.277778%\n532: 1.111111%\n533: 3.333333%\n534: 17.222222%\n535: 6.949495%\n536: 12.535714%\n537: 12.579365%\n538: 5.000000%\n539: 1.964286%\n540: 5.000000%\n541: 2.857143%\n542: 18.364469%\n543: 5.587662%\n544: 2.083333%\n545: 1.623377%\n546: 0.000000%\n547: 3.333333%\n548: 1.111111%\n549: 0.000000%\n550: 14.088023%\n551: 4.047619%\n552: 10.000000%\n553: 7.619048%\n554: 9.935897%\n555: 4.242424%\n556: 13.250000%\n557: 2.020202%\n558: 0.000000%\n559: 2.500000%\n560: 5.833333%\n561: 10.769231%\n562: 4.671717%\n563: 5.000000%\n564: 2.500000%\n565: 3.333333%\n566: 2.909091%\n567: 13.333333%\n568: 0.769231%\n569: 0.833333%\n570: 2.857143%\n571: 7.916667%\n572: 2.307692%\n573: 4.761905%\n574: 0.000000%\n575: 8.888889%\n576: 1.547619%\n577: 5.416667%\n578: 3.678322%\n579: 0.000000%\n580: 1.666667%\n581: 1.000000%\n582: 7.664336%\n583: 1.111111%\n584: 1.818182%\n585: 1.000000%\n586: 2.083333%\n587: 3.750000%\n588: 6.250000%\n589: 8.095238%\n590: 4.333333%\n591: 2.777778%\n592: 0.000000%\n593: 6.666667%\n594: 4.242424%\n595: 2.000000%\n596: 4.381313%\n597: 3.333333%\n598: 0.000000%\n599: 0.000000%\n600: 0.000000%\n601: 3.000000%\n602: 0.000000%\n603: 1.666667%\n604: 0.833333%\n605: 13.789683%\n606: 4.242424%\n607: 5.000000%\n608: 3.333333%\n609: 3.111111%\n610: 0.000000%\n611: 5.555556%\n612: 2.447552%\n613: 0.000000%\n614: 1.547619%\n615: 8.333333%\n616: 14.222222%\n617: 2.000000%\n618: 14.000000%\n619: 4.307692%\n620: 1.428571%\n621: 5.666667%\n622: 0.000000%\n623: 7.159091%\n624: 2.222222%\n625: 3.333333%\n626: 0.000000%\n627: 2.000000%\n628: 3.333333%\n629: 13.333333%\n630: 1.666667%\n631: 4.444444%\n632: 2.777778%\n633: 0.000000%\n634: 7.083333%\n635: 4.000000%\n636: 7.777778%\n637: 0.000000%\n638: 4.761905%\n639: 7.916667%\n640: 0.000000%\n641: 0.000000%\n642: 0.909091%\n643: 2.111111%\n644: 0.000000%\n645: 0.000000%\n646: 0.000000%\n647: 1.666667%\n648: 0.714286%\n649: 0.000000%\n650: 1.111111%\n651: 10.769231%\n652: 0.714286%\n653: 6.666667%\n654: 3.636364%\n655: 4.761905%\n656: 3.760684%\n657: 8.380952%\n658: 20.000000%\n659: 3.428571%\n660: 3.222222%\n661: 5.555556%\n662: 5.555556%\n663: 0.000000%\n664: 0.000000%\n665: 4.242424%\n666: 3.472222%\n667: 6.964286%\n668: 1.666667%\n669: 4.761905%\n670: 3.333333%\n671: 0.000000%\n672: 2.678571%\n673: 10.666667%\n674: 4.333333%\n675: 1.111111%\n676: 5.416667%\n677: 4.166667%\n678: 3.333333%\n679: 11.352564%\n680: 3.666667%\n681: 0.000000%\n682: 5.000000%\n683: 2.500000%\n684: 6.000000%\n685: 0.000000%\n686: 5.555556%\n687: 3.068182%\n688: 0.000000%\n689: 0.769231%\n690: 1.666667%\n691: 3.333333%\n692: 2.000000%\n693: 8.666667%\n694: 4.444444%\n695: 0.000000%\n696: 1.000000%\n697: 0.000000%\n698: 2.000000%\n699: 0.000000%\n700: 8.154762%\n701: 6.666667%\n702: 4.900794%\n703: 1.111111%\n704: 2.111111%\n705: 3.333333%\n706: 0.000000%\n707: 2.019231%\n708: 12.857143%\n709: 2.000000%\n710: 0.000000%\n711: 8.428571%\n712: 0.000000%\n713: 2.197802%\n714: 0.000000%\n715: 11.666667%\n716: 0.000000%\n717: 8.142857%\n718: 8.333333%\n719: 3.131313%\n720: 0.909091%\n721: 0.000000%\n722: 0.000000%\n723: 0.000000%\n724: 2.000000%\n725: 0.833333%\n726: 0.000000%\n727: 1.111111%\n728: 7.252747%\n729: 2.539683%\n730: 3.750000%\n731: 1.666667%\n732: 1.250000%\n733: 0.000000%\n734: 2.500000%\n735: 0.000000%\n736: 3.333333%\n737: 0.000000%\n738: 2.500000%\n739: 3.742424%\n740: 6.666667%\n741: 0.714286%\n742: 1.428571%\n743: 0.000000%\n744: 4.000000%\n745: 2.000000%\n746: 13.793651%\n747: 6.666667%\n748: 1.000000%\n749: 1.666667%\n750: 5.555556%\n751: 3.650794%\n752: 1.111111%\n753: 0.000000%\n754: 0.000000%\n755: 11.333333%\n756: 6.666667%\n757: 4.444444%\n758: 8.166667%\n759: 9.797980%\n760: 0.000000%\n761: 7.261905%\n762: 0.000000%\n763: 6.666667%\n764: 3.333333%\n765: 1.428571%\n766: 4.047619%\n767: 7.440476%\n768: 0.000000%\n769: 1.547619%\n770: 0.000000%\n771: 0.000000%\n772: 4.345238%\n773: 2.857143%\n774: 0.000000%\n775: 2.142857%\n776: 2.916667%\n777: 0.000000%\n778: 2.500000%\n779: 2.222222%\n780: 7.857143%\n781: 1.538462%\n782: 0.000000%\n783: 0.000000%\n784: 5.833333%\n785: 2.222222%\n786: 5.333333%\n787: 16.666667%\n788: 5.000000%\n789: 0.000000%\n790: 3.750000%\n791: 0.000000%\n792: 1.666667%\n793: 3.333333%\n794: 6.666667%\n795: 9.444444%\n796: 7.714286%\n797: 6.666667%\n798: 6.666667%\n799: 11.269841%\n800: 3.766234%\n801: 0.000000%\n802: 1.111111%\n803: 7.678571%\n804: 2.000000%\n805: 3.333333%\n806: 2.500000%\n807: 0.000000%\n808: 5.333333%\n809: 1.250000%\n810: 3.333333%\n811: 7.916667%\n812: 0.000000%\n813: 0.000000%\n814: 2.142857%\n815: 2.857143%\n816: 0.000000%\n817: 4.102564%\n818: 3.333333%\n819: 1.111111%\n820: 0.000000%\n821: 4.871795%\n822: 0.000000%\n823: 5.670996%\n824: 0.909091%\n825: 15.111111%\n826: 0.000000%\n827: 0.909091%\n828: 6.666667%\n829: 0.000000%\n830: 2.000000%\n831: 11.022727%\n832: 8.166667%\n833: 3.095238%\n834: 6.761905%\n835: 3.083333%\n836: 3.333333%\n837: 1.250000%\n838: 0.714286%\n839: 6.000000%\n840: 2.967033%\n841: 1.111111%\n842: 0.000000%\n843: 1.000000%\n844: 6.111111%\n845: 9.363636%\n846: 7.380952%\n847: 3.333333%\n848: 0.000000%\n849: 2.539683%\n850: 11.666667%\n851: 0.000000%\n852: 1.909091%\n853: 10.000000%\n854: 0.000000%\n855: 3.095238%\n856: 8.500000%\n857: 1.250000%\n858: 0.000000%\n859: 0.714286%\n860: 5.000000%\n861: 0.000000%\n862: 0.000000%\n863: 0.000000%\n864: 4.444444%\n865: 1.742424%\n866: 5.761905%\n867: 3.750000%\n868: 0.000000%\n869: 4.957265%\n870: 4.583333%\n871: 2.727273%\n872: 0.000000%\n873: 6.000000%\n874: 13.333333%\n875: 4.500000%\n876: 6.583333%\n877: 4.444444%\n878: 1.428571%\n879: 6.666667%\n880: 5.213675%\n881: 0.000000%\n882: 2.111111%\n883: 16.011905%\n884: 0.000000%\n885: 5.714286%\n886: 4.107143%\n887: 0.000000%\n888: 2.500000%\n889: 0.769231%\n890: 4.583333%\n891: 0.909091%\n892: 4.166667%\n893: 2.727273%\n894: 15.795455%\n895: 0.000000%\n896: 6.444444%\n897: 0.000000%\n898: 1.428571%\n899: 5.285714%\n900: 1.000000%\n901: 0.000000%\n902: 0.000000%\n903: 3.333333%\n904: 0.000000%\n905: 0.000000%\n906: 0.000000%\n907: 0.000000%\n908: 0.000000%\n909: 0.000000%\n910: 0.000000%\n911: 10.000000%\n912: 1.250000%\n913: 0.000000%\n914: 6.666667%\n915: 1.666667%\n916: 0.000000%\n917: 0.000000%\n918: 0.000000%\n919: 0.000000%\n920: 0.000000%\n921: 0.000000%\n922: 0.000000%\n923: 0.000000%\n924: 0.000000%\n925: 0.000000%\n926: 0.000000%\n927: 0.000000%\n928: 0.000000%\n929: 0.000000%\n930: 0.000000%\n931: 0.000000%\n932: 0.000000%\n933: 0.000000%\n934: 0.000000%\n935: 0.000000%\n936: 0.909091%\n937: 6.666667%\n938: 1.000000%\n939: 1.111111%\n940: 0.000000%\n941: 0.000000%\n942: 0.000000%\n943: 0.000000%\n944: 0.000000%\n945: 0.769231%\n946: 0.000000%\n947: 0.000000%\n948: 0.000000%\n949: 0.000000%\n950: 0.000000%\n951: 0.714286%\n952: 0.000000%\n953: 0.000000%\n954: 0.000000%\n955: 0.000000%\n956: 0.000000%\n957: 5.333333%\n958: 0.000000%\n959: 1.428571%\n960: 0.000000%\n961: 2.727273%\n962: 0.000000%\n963: 0.000000%\n964: 0.833333%\n965: 0.909091%\n966: 0.714286%\n967: 0.000000%\n968: 0.000000%\n969: 2.111111%\n970: 0.000000%\n971: 0.769231%\n972: 3.333333%\n973: 0.000000%\n974: 1.666667%\n975: 1.111111%\n976: 0.000000%\n977: 0.000000%\n978: 1.111111%\n979: 0.000000%\n980: 0.000000%\n981: 1.666667%\n982: 0.000000%\n983: 1.000000%\n984: 0.000000%\n985: 0.000000%\n986: 1.250000%\n987: 2.000000%\n988: 0.000000%\n989: 0.000000%\n990: 4.000000%\n991: 0.000000%\n992: 0.000000%\n993: 7.619048%\n994: 0.000000%\n995: 0.000000%\n996: 0.000000%\n997: 1.111111%\n998: 0.000000%\n999: 0.000000%\n1000: 0.000000%\n1001: 0.000000%\n1002: 0.000000%\n1003: 1.428571%\n1004: 0.000000%\n1005: 3.333333%\n1006: 1.111111%\n1007: 0.000000%\n1008: 5.000000%\n1009: 0.000000%\n1010: 0.000000%\n1011: 0.000000%\n1012: 1.666667%\n1013: 11.861472%\n1014: 0.000000%\n1015: 0.000000%\n1016: 1.111111%\n1017: 1.111111%\n1018: 0.000000%\n1019: 0.000000%\n1020: 0.000000%\n1021: 1.111111%\n1022: 0.833333%\n1023: 6.666667%\n1024: 0.000000%\n1025: 0.000000%\n1026: 0.000000%\n1027: 10.000000%\n1028: 3.333333%\n1029: 1.111111%\n1030: 0.000000%\n1031: 1.250000%\n1032: 0.000000%\n1033: 0.000000%\n1034: 0.000000%\n1035: 0.000000%\n1036: 0.000000%\n1037: 0.000000%\n1038: 0.000000%\n1039: 0.000000%\n1040: 0.000000%\n1041: 2.916667%\n1042: 0.000000%\n1043: 0.000000%\n1044: 0.000000%\n1045: 0.000000%\n1046: 6.666667%\n1047: 0.000000%\n1048: 0.000000%\n1049: 0.000000%\n1050: 0.000000%\n1051: 11.666667%\n1052: 0.000000%\n1053: 2.000000%\n1054: 0.000000%\n1055: 0.000000%\n1056: 6.666667%\n1057: 0.833333%\n1058: 0.000000%\n1059: 0.000000%\n1060: 0.000000%\n1061: 0.000000%\n1062: 3.333333%\n1063: 3.250000%\n1064: 0.000000%\n1065: 0.000000%\n1066: 4.242424%\n1067: 0.000000%\n1068: 0.000000%\n"}, {"output_type": "stream", "name": "stdout", "text": "1069: 0.000000%\n1070: 1.666667%\n1071: 1.111111%\n1072: 2.000000%\n1073: 2.142857%\n1074: 0.000000%\n1075: 6.666667%\n1076: 4.435897%\n1077: 0.000000%\n1078: 1.000000%\n1079: 0.000000%\n1080: 0.000000%\n1081: 0.000000%\n1082: 0.000000%\n1083: 0.000000%\n1084: 3.333333%\n1085: 3.333333%\n1086: 0.000000%\n1087: 10.714286%\n1088: 1.538462%\n1089: 0.000000%\n1090: 1.111111%\n1091: 0.000000%\n1092: 4.102564%\n1093: 0.000000%\n1094: 0.000000%\n1095: 0.000000%\n1096: 3.333333%\n1097: 0.000000%\n1098: 6.666667%\n1099: 0.000000%\n1100: 0.000000%\n1101: 5.000000%\n1102: 0.000000%\n1103: 3.214286%\n1104: 0.000000%\n1105: 2.000000%\n1106: 0.000000%\n1107: 1.111111%\n1108: 0.000000%\n1109: 0.000000%\n1110: 0.000000%\n1111: 0.000000%\n1112: 1.111111%\n1113: 0.000000%\n1114: 3.333333%\n1115: 0.000000%\n1116: 1.111111%\n1117: 0.000000%\n1118: 0.000000%\n1119: 1.742424%\n1120: 0.000000%\n1121: 6.666667%\n1122: 0.000000%\n1123: 0.000000%\n1124: 0.000000%\n1125: 0.000000%\n1126: 0.000000%\n1127: 0.000000%\n1128: 0.714286%\n1129: 0.000000%\n1130: 7.333333%\n1131: 4.000000%\n1132: 5.000000%\n1133: 0.000000%\n1134: 0.000000%\n1135: 1.666667%\n1136: 0.000000%\n1137: 0.000000%\n1138: 0.000000%\n1139: 0.000000%\n1140: 0.000000%\n1141: 3.333333%\n1142: 0.000000%\n1143: 0.000000%\n1144: 5.714286%\n1145: 1.250000%\n1146: 6.000000%\n1147: 0.000000%\n1148: 0.000000%\n1149: 2.222222%\n1150: 0.000000%\n1151: 0.000000%\n1152: 0.000000%\n1153: 0.000000%\n1154: 0.714286%\n1155: 3.269231%\n1156: 0.000000%\n1157: 6.666667%\n1158: 0.000000%\n1159: 2.222222%\n1160: 0.000000%\n1161: 3.333333%\n1162: 0.000000%\n1163: 0.000000%\n1164: 0.000000%\n1165: 0.000000%\n1166: 0.000000%\n1167: 2.857143%\n1168: 0.000000%\n1169: 6.666667%\n1170: 0.000000%\n1171: 3.333333%\n1172: 0.000000%\n1173: 0.000000%\n1174: 3.333333%\n1175: 0.000000%\n1176: 1.250000%\n1177: 1.111111%\n1178: 0.000000%\n1179: 0.000000%\n1180: 1.111111%\n1181: 1.666667%\n1182: 0.000000%\n1183: 1.666667%\n1184: 0.000000%\n1185: 3.838384%\n1186: 0.000000%\n1187: 0.000000%\n1188: 0.000000%\n1189: 0.000000%\n1190: 0.000000%\n1191: 0.000000%\n1192: 0.000000%\n1193: 1.111111%\n1194: 0.000000%\n1195: 3.333333%\n1196: 0.000000%\n1197: 2.500000%\n1198: 2.222222%\n1199: 0.909091%\n1200: 0.714286%\n1201: 1.000000%\n1202: 3.333333%\n1203: 8.928571%\n1204: 0.000000%\n1205: 0.000000%\n1206: 0.000000%\n1207: 0.000000%\n1208: 1.000000%\n1209: 2.000000%\n1210: 1.000000%\n1211: 0.000000%\n1212: 0.000000%\n1213: 1.666667%\n1214: 0.833333%\n1215: 6.666667%\n1216: 0.000000%\n1217: 0.000000%\n1218: 0.000000%\n1219: 0.000000%\n1220: 0.000000%\n1221: 0.000000%\n1222: 3.750000%\n1223: 10.000000%\n1224: 2.000000%\n1225: 1.428571%\n1226: 5.000000%\n1227: 0.000000%\n1228: 6.666667%\n1229: 0.000000%\n1230: 2.222222%\n1231: 0.000000%\n1232: 0.000000%\n1233: 3.333333%\n1234: 0.000000%\n1235: 1.666667%\n1236: 10.000000%\n1237: 0.000000%\n1238: 0.000000%\n1239: 0.000000%\n1240: 0.000000%\n1241: 0.000000%\n1242: 0.000000%\n1243: 1.111111%\n1244: 0.000000%\n1245: 0.000000%\n1246: 1.666667%\n1247: 0.000000%\n1248: 0.000000%\n1249: 1.428571%\n1250: 0.000000%\n1251: 0.000000%\n1252: 0.000000%\n1253: 10.000000%\n1254: 0.000000%\n1255: 0.000000%\n1256: 0.000000%\n1257: 0.000000%\n1258: 0.000000%\n1259: 0.000000%\n1260: 0.714286%\n1261: 0.000000%\n1262: 0.000000%\n1263: 0.000000%\n1264: 1.111111%\n1265: 0.000000%\n1266: 3.333333%\n1267: 5.000000%\n1268: 0.000000%\n1269: 7.777778%\n1270: 1.538462%\n1271: 0.000000%\n1272: 0.714286%\n1273: 8.142857%\n1274: 0.000000%\n1275: 0.000000%\n1276: 4.166667%\n1277: 4.583333%\n1278: 0.714286%\n1279: 1.666667%\n1280: 0.000000%\n1281: 1.666667%\n1282: 2.000000%\n1283: 0.000000%\n1284: 6.666667%\n1285: 0.000000%\n1286: 2.777778%\n1287: 0.000000%\n1288: 0.000000%\n1289: 0.000000%\n1290: 0.000000%\n1291: 6.666667%\n1292: 0.000000%\n1293: 0.000000%\n1294: 0.000000%\n1295: 0.000000%\n1296: 3.333333%\n1297: 0.000000%\n1298: 0.000000%\n1299: 1.111111%\n1300: 0.833333%\n1301: 3.333333%\n1302: 0.000000%\n1303: 0.000000%\n1304: 0.000000%\n1305: 0.000000%\n1306: 1.666667%\n1307: 0.000000%\n1308: 0.000000%\n1309: 0.000000%\n1310: 7.777778%\n1311: 0.000000%\n1312: 0.000000%\n1313: 0.000000%\n1314: 4.000000%\n1315: 0.000000%\n1316: 0.000000%\n1317: 0.000000%\n1318: 0.000000%\n1319: 0.000000%\n1320: 6.190476%\n1321: 0.000000%\n1322: 1.428571%\n1323: 2.222222%\n1324: 5.555556%\n1325: 3.333333%\n1326: 0.000000%\n1327: 0.000000%\n1328: 1.602564%\n1329: 1.428571%\n1330: 0.000000%\n1331: 0.000000%\n1332: 3.333333%\n1333: 0.000000%\n1334: 0.000000%\n1335: 3.333333%\n1336: 1.666667%\n1337: 0.000000%\n1338: 0.000000%\n1339: 0.000000%\n1340: 1.111111%\n1341: 0.833333%\n1342: 1.250000%\n1343: 0.000000%\n1344: 0.000000%\n1345: 0.000000%\n1346: 0.000000%\n1347: 0.000000%\n1348: 0.000000%\n1349: 0.000000%\n1350: 0.000000%\n1351: 0.000000%\n1352: 0.000000%\n1353: 0.000000%\n1354: 0.000000%\n1355: 0.000000%\n1356: 0.000000%\n1357: 0.000000%\n1358: 0.769231%\n1359: 0.000000%\n1360: 0.000000%\n1361: 0.000000%\n1362: 0.000000%\n1363: 0.000000%\n1364: 0.000000%\n1365: 0.000000%\n1366: 0.000000%\n1367: 0.000000%\n1368: 0.000000%\n1369: 0.000000%\n1370: 0.000000%\n1371: 0.000000%\n1372: 0.000000%\n1373: 0.000000%\n1374: 0.000000%\n1375: 0.000000%\n1376: 0.000000%\n1377: 0.000000%\n1378: 0.000000%\n1379: 0.000000%\n1380: 0.000000%\n1381: 0.000000%\n1382: 0.000000%\n1383: 0.000000%\n1384: 0.000000%\n1385: 0.000000%\n1386: 0.000000%\n1387: 0.000000%\n1388: 0.000000%\n1389: 0.000000%\n1390: 1.666667%\n1391: 0.000000%\n1392: 0.000000%\n1393: 0.000000%\n1394: 0.000000%\n1395: 0.000000%\n1396: 2.916667%\n1397: 1.666667%\n1398: 4.285714%\n1399: 0.000000%\n1400: 0.000000%\n1401: 0.000000%\n1402: 0.000000%\n1403: 0.000000%\n1404: 0.000000%\n1405: 0.000000%\n1406: 3.333333%\n1407: 0.000000%\n1408: 0.000000%\n1409: 1.111111%\n1410: 1.250000%\n1411: 0.000000%\n1412: 1.666667%\n1413: 0.000000%\n1414: 0.000000%\n1415: 0.000000%\n1416: 4.000000%\n1417: 0.000000%\n1418: 0.000000%\n1419: 3.333333%\n1420: 0.714286%\n1421: 0.000000%\n1422: 0.000000%\n1423: 0.714286%\n1424: 0.909091%\n1425: 0.000000%\n1426: 0.000000%\n1427: 0.000000%\n1428: 0.000000%\n1429: 3.333333%\n1430: 1.666667%\n1431: 0.000000%\n1432: 0.000000%\n1433: 1.666667%\n1434: 0.000000%\n1435: 0.000000%\n1436: 2.500000%\n1437: 0.000000%\n1438: 0.000000%\n1439: 6.666667%\n1440: 0.000000%\n1441: 0.833333%\n1442: 0.000000%\n1443: 2.000000%\n1444: 0.000000%\n1445: 10.000000%\n1446: 0.000000%\n1447: 0.000000%\n1448: 0.000000%\n1449: 0.000000%\n1450: 0.000000%\n1451: 0.000000%\n1452: 0.000000%\n1453: 0.000000%\n1454: 0.000000%\n1455: 0.000000%\n1456: 0.000000%\n1457: 0.000000%\n1458: 0.000000%\n1459: 0.000000%\n1460: 0.000000%\n1461: 0.000000%\n1462: 0.000000%\n1463: 0.000000%\n1464: 0.000000%\n1465: 0.000000%\n1466: 0.000000%\n1467: 0.000000%\n1468: 0.000000%\n1469: 0.000000%\n1470: 3.333333%\n1471: 0.000000%\n1472: 0.000000%\n1473: 3.333333%\n1474: 0.000000%\n1475: 0.000000%\n1476: 0.000000%\n1477: 0.000000%\n1478: 0.000000%\n1479: 0.000000%\n1480: 0.000000%\n1481: 0.000000%\n1482: 0.000000%\n1483: 0.000000%\n1484: 0.000000%\n1485: 0.000000%\n1486: 0.000000%\n1487: 0.000000%\n1488: 0.000000%\n1489: 0.000000%\n1490: 0.000000%\n1491: 0.000000%\n1492: 0.000000%\n1493: 0.000000%\n1494: 0.000000%\n1495: 0.000000%\n1496: 0.000000%\n1497: 0.000000%\n1498: 0.000000%\n1499: 0.000000%\n1500: 0.000000%\n1501: 1.250000%\n1502: 0.000000%\n1503: 0.000000%\n1504: 0.000000%\n1505: 0.000000%\n1506: 0.000000%\n1507: 0.000000%\n1508: 0.000000%\n1509: 1.666667%\n1510: 0.833333%\n1511: 0.000000%\n1512: 0.000000%\n1513: 0.000000%\n1514: 6.666667%\n1515: 2.250000%\n1516: 3.333333%\n1517: 0.000000%\n1518: 0.000000%\n1519: 0.000000%\n1520: 0.000000%\n1521: 0.000000%\n1522: 0.000000%\n1523: 0.000000%\n1524: 0.000000%\n1525: 0.000000%\n1526: 0.000000%\n1527: 0.000000%\n1528: 0.000000%\n1529: 0.000000%\n1530: 0.000000%\n1531: 0.000000%\n1532: 0.000000%\n1533: 0.000000%\n1534: 0.000000%\n1535: 0.000000%\n1536: 0.000000%\n1537: 0.000000%\n1538: 0.000000%\n1539: 0.000000%\n1540: 0.000000%\n1541: 3.333333%\n1542: 0.000000%\n1543: 0.000000%\n1544: 0.000000%\n1545: 0.000000%\n1546: 6.666667%\n1547: 0.000000%\n1548: 0.000000%\n1549: 0.000000%\n1550: 0.000000%\n1551: 0.000000%\n1552: 0.000000%\n1553: 0.000000%\n1554: 0.000000%\n1555: 0.000000%\n1556: 0.000000%\n1557: 0.000000%\n1558: 0.000000%\n1559: 0.000000%\n1560: 3.333333%\n1561: 0.000000%\n1562: 0.000000%\n1563: 3.333333%\n1564: 0.000000%\n1565: 0.000000%\n1566: 0.000000%\n1567: 6.666667%\n1568: 0.000000%\n1569: 0.000000%\n1570: 3.333333%\n1571: 0.000000%\n1572: 0.000000%\n1573: 1.666667%\n1574: 0.000000%\n1575: 0.000000%\n1576: 0.000000%\n1577: 0.000000%\n1578: 2.769231%\n1579: 0.000000%\n1580: 0.000000%\n"}, {"output_type": "stream", "name": "stdout", "text": "1581: 0.000000%\n1582: 0.000000%\n1583: 0.000000%\n1584: 5.444444%\n1585: 0.000000%\n1586: 0.000000%\n1587: 0.000000%\n1588: 0.000000%\n1589: 1.111111%\n1590: 3.333333%\n1591: 0.000000%\n1592: 0.000000%\n1593: 0.000000%\n1594: 0.000000%\n1595: 0.000000%\n1596: 0.000000%\n1597: 0.000000%\n1598: 0.000000%\n1599: 0.000000%\n1600: 0.000000%\n1601: 0.000000%\n1602: 0.000000%\n1603: 0.000000%\n1604: 0.000000%\n1605: 0.000000%\n1606: 3.333333%\n1607: 0.000000%\n1608: 3.333333%\n1609: 0.000000%\n1610: 0.000000%\n1611: 0.000000%\n1612: 0.000000%\n1613: 0.000000%\n1614: 0.000000%\n1615: 0.000000%\n1616: 0.000000%\n1617: 0.000000%\n1618: 0.000000%\n1619: 1.250000%\n1620: 0.000000%\n1621: 2.000000%\n1622: 0.000000%\n1623: 0.000000%\n1624: 0.000000%\n1625: 0.000000%\n1626: 0.000000%\n1627: 0.000000%\n1628: 0.000000%\n1629: 0.000000%\n1630: 0.000000%\n1631: 0.000000%\n1632: 0.000000%\n1633: 0.000000%\n1634: 0.000000%\n1635: 0.000000%\n1636: 0.000000%\n1637: 0.000000%\n1638: 0.000000%\n1639: 1.111111%\n1640: 1.250000%\n1641: 0.000000%\n1642: 0.000000%\n1643: 0.000000%\n1644: 0.000000%\n1645: 0.000000%\n1646: 0.000000%\n1647: 0.000000%\n1648: 0.000000%\n1649: 5.000000%\n1650: 0.000000%\n1651: 0.000000%\n1652: 0.000000%\n1653: 0.000000%\n1654: 0.000000%\n1655: 0.000000%\n1656: 0.000000%\n1657: 0.000000%\n1658: 2.678571%\n1659: 1.000000%\n1660: 0.000000%\n1661: 0.000000%\n1662: 1.666667%\n1663: 0.000000%\n1664: 0.000000%\n1665: 0.000000%\n1666: 4.102564%\n1667: 0.000000%\n1668: 0.000000%\n1669: 0.000000%\n1670: 0.000000%\n1671: 0.000000%\n1672: 3.333333%\n1673: 0.000000%\n1674: 0.000000%\n1675: 0.000000%\n1676: 3.333333%\n1677: 0.000000%\n1678: 0.000000%\n1679: 0.000000%\n1680: 0.000000%\n1681: 0.000000%\n1682: 0.000000%\n1683: 0.000000%\n1684: 0.000000%\n1685: 0.000000%\n1686: 0.000000%\n1687: 0.000000%\n1688: 0.000000%\n1689: 0.000000%\n1690: 0.000000%\n1691: 0.000000%\n1692: 0.000000%\n1693: 0.000000%\n1694: 1.666667%\n1695: 0.000000%\n1696: 0.000000%\n1697: 0.000000%\n1698: 3.333333%\n1699: 0.000000%\n1700: 0.000000%\n1701: 0.000000%\n1702: 0.000000%\n1703: 0.000000%\n1704: 0.000000%\n1705: 2.337662%\n1706: 3.333333%\n1707: 0.000000%\n1708: 0.000000%\n1709: 0.000000%\n1710: 0.000000%\n1711: 3.333333%\n1712: 0.000000%\n1713: 0.000000%\n1714: 0.000000%\n1715: 0.000000%\n1716: 0.000000%\n1717: 0.000000%\n1718: 0.000000%\n1719: 0.000000%\n1720: 0.000000%\n1721: 0.000000%\n1722: 0.000000%\n1723: 0.000000%\n1724: 0.000000%\n1725: 8.333333%\n1726: 0.000000%\n1727: 0.000000%\n1728: 0.000000%\n1729: 0.000000%\n1730: 0.000000%\n1731: 0.000000%\n1732: 0.000000%\n1733: 0.000000%\n1734: 0.000000%\n1735: 0.000000%\n1736: 0.000000%\n1737: 0.000000%\n1738: 0.000000%\n1739: 0.000000%\n1740: 0.000000%\n1741: 0.000000%\n1742: 0.000000%\n1743: 0.000000%\n1744: 10.000000%\n1745: 0.000000%\n1746: 0.000000%\n1747: 0.000000%\n1748: 3.333333%\n1749: 0.769231%\n1750: 0.000000%\n1751: 1.666667%\n1752: 0.000000%\n1753: 3.333333%\n1754: 0.000000%\n1755: 1.428571%\n1756: 1.000000%\n1757: 0.000000%\n1758: 0.000000%\n1759: 0.000000%\n1760: 0.000000%\n1761: 1.111111%\n1762: 0.000000%\n1763: 2.000000%\n1764: 0.000000%\n1765: 0.000000%\n1766: 0.000000%\n1767: 0.000000%\n1768: 0.000000%\n1769: 0.000000%\n1770: 3.333333%\n1771: 0.000000%\n1772: 0.000000%\n1773: 0.000000%\n1774: 0.000000%\n1775: 0.000000%\n1776: 0.000000%\n1777: 0.000000%\n1778: 1.428571%\n1779: 0.000000%\n1780: 0.000000%\n1781: 0.000000%\n1782: 3.333333%\n1783: 0.000000%\n1784: 0.000000%\n1785: 0.000000%\n1786: 0.000000%\n1787: 0.000000%\n1788: 0.000000%\n1789: 0.000000%\n1790: 0.769231%\n1791: 6.666667%\n1792: 0.000000%\n1793: 0.000000%\n1794: 0.000000%\n1795: 0.000000%\n1796: 0.000000%\n1797: 0.000000%\n1798: 0.000000%\n1799: 0.000000%\n1800: 0.000000%\n1801: 0.000000%\n1802: 0.909091%\n1803: 0.000000%\n1804: 0.000000%\n1805: 1.250000%\n1806: 0.000000%\n1807: 0.000000%\n1808: 0.833333%\n1809: 0.000000%\n1810: 0.000000%\n1811: 0.000000%\n1812: 0.000000%\n1813: 0.000000%\n1814: 0.000000%\n1815: 0.000000%\n1816: 0.000000%\n1817: 0.000000%\n1818: 0.000000%\n1819: 3.333333%\n1820: 0.000000%\n1821: 0.000000%\n1822: 1.000000%\n1823: 0.000000%\n1824: 0.000000%\n1825: 0.000000%\n1826: 0.000000%\n1827: 3.333333%\n1828: 0.000000%\n1829: 0.000000%\n1830: 0.000000%\n1831: 0.000000%\n1832: 0.000000%\n1833: 0.000000%\n1834: 0.000000%\n1835: 0.000000%\n1836: 0.000000%\n1837: 0.000000%\n1838: 1.000000%\n1839: 0.000000%\n1840: 0.000000%\n1841: 0.000000%\n1842: 0.000000%\n1843: 0.000000%\n1844: 0.000000%\n1845: 0.000000%\n1846: 0.000000%\n1847: 0.000000%\n1848: 0.000000%\n1849: 0.000000%\n1850: 0.000000%\n1851: 0.000000%\n1852: 0.000000%\n1853: 0.000000%\n1854: 0.000000%\n1855: 0.000000%\n1856: 0.000000%\n1857: 0.000000%\n1858: 0.000000%\n1859: 0.000000%\n1860: 0.000000%\n1861: 0.000000%\n1862: 0.000000%\n1863: 0.000000%\n1864: 0.000000%\n1865: 0.000000%\n1866: 0.000000%\n1867: 0.000000%\n1868: 0.000000%\n1869: 0.000000%\n1870: 0.000000%\n1871: 0.000000%\n1872: 0.000000%\n1873: 0.000000%\n1874: 0.000000%\n1875: 0.000000%\n1876: 0.000000%\n1877: 0.000000%\n1878: 0.000000%\n1879: 0.000000%\n1880: 0.000000%\n1881: 0.000000%\n1882: 0.000000%\n1883: 0.000000%\n1884: 0.000000%\n1885: 1.111111%\n1886: 0.000000%\n1887: 1.000000%\n1888: 0.000000%\n1889: 1.111111%\n1890: 0.000000%\n1891: 0.000000%\n1892: 0.000000%\n1893: 0.000000%\n1894: 0.000000%\n1895: 0.000000%\n1896: 0.000000%\n1897: 0.000000%\n1898: 0.000000%\n1899: 0.000000%\n1900: 0.000000%\n1901: 0.000000%\n1902: 0.000000%\n1903: 0.000000%\n1904: 0.000000%\n1905: 0.000000%\n1906: 1.818182%\n1907: 0.000000%\n1908: 0.000000%\n1909: 0.714286%\n1910: 0.000000%\n1911: 0.000000%\n1912: 0.000000%\n1913: 0.000000%\n1914: 0.000000%\n1915: 0.000000%\n1916: 0.000000%\n1917: 0.000000%\n1918: 0.000000%\n1919: 0.000000%\n1920: 0.000000%\n1921: 0.000000%\n1922: 0.000000%\n1923: 0.000000%\n1924: 0.000000%\n1925: 0.000000%\n1926: 0.000000%\n1927: 0.000000%\n1928: 0.000000%\n1929: 0.000000%\n1930: 2.000000%\n1931: 0.000000%\n1932: 0.000000%\n1933: 0.000000%\n1934: 0.000000%\n1935: 0.000000%\n1936: 1.000000%\n1937: 0.000000%\n1938: 0.833333%\n1939: 0.000000%\n1940: 0.000000%\n1941: 0.769231%\n1942: 0.000000%\n1943: 0.000000%\n1944: 2.000000%\n1945: 0.000000%\n1946: 0.000000%\n1947: 0.000000%\n1948: 0.000000%\n1949: 0.000000%\n1950: 0.000000%\n1951: 0.000000%\n1952: 0.000000%\n1953: 0.000000%\n1954: 0.000000%\n1955: 0.000000%\n1956: 1.666667%\n1957: 0.000000%\n1958: 0.000000%\n1959: 0.000000%\n1960: 0.000000%\n1961: 0.000000%\n1962: 0.000000%\n1963: 0.000000%\n1964: 0.000000%\n1965: 2.500000%\n1966: 0.000000%\n1967: 0.000000%\n1968: 0.833333%\n1969: 0.000000%\n1970: 0.000000%\n1971: 0.000000%\n1972: 0.000000%\n1973: 0.000000%\n1974: 0.000000%\n1975: 0.000000%\n1976: 0.000000%\n1977: 0.000000%\n1978: 0.000000%\n1979: 0.000000%\n1980: 1.250000%\n1981: 0.000000%\n1982: 0.000000%\n1983: 0.000000%\n1984: 0.000000%\n1985: 0.000000%\n1986: 0.000000%\n1987: 0.000000%\n1988: 0.000000%\n1989: 0.000000%\n1990: 0.000000%\n1991: 0.000000%\n1992: 0.000000%\n1993: 0.000000%\n1994: 0.000000%\n1995: 0.000000%\n1996: 0.833333%\n1997: 0.000000%\n1998: 0.000000%\n1999: 0.000000%\n2000: 0.000000%\n2001: 0.000000%\n2002: 0.000000%\n2003: 0.000000%\n2004: 0.000000%\n2005: 0.000000%\n2006: 0.000000%\n2007: 0.000000%\n2008: 0.000000%\n2009: 0.000000%\n2010: 0.833333%\n2011: 0.000000%\n2012: 0.000000%\n2013: 0.000000%\n2014: 0.000000%\n2015: 0.000000%\n2016: 0.000000%\n2017: 0.000000%\n2018: 0.000000%\n2019: 0.000000%\n2020: 0.000000%\n2021: 0.000000%\n2022: 0.000000%\n2023: 0.000000%\n2024: 0.000000%\n2025: 0.000000%\n2026: 0.000000%\n2027: 0.000000%\n2028: 0.000000%\n2029: 0.000000%\n2030: 0.000000%\n2031: 0.000000%\n2032: 0.000000%\n2033: 0.000000%\n2034: 0.000000%\n2035: 0.000000%\n2036: 0.000000%\n2037: 0.000000%\n2038: 0.000000%\n2039: 0.909091%\n2040: 1.111111%\n2041: 3.333333%\n2042: 0.000000%\n2043: 1.428571%\n2044: 0.000000%\n2045: 1.111111%\n2046: 0.000000%\n2047: 0.000000%\n2048: 0.000000%\n2049: 0.000000%\n2050: 0.000000%\n2051: 0.833333%\n2052: 0.000000%\n2053: 0.000000%\n2054: 0.000000%\n2055: 0.000000%\n2056: 0.000000%\n2057: 1.909091%\n2058: 0.000000%\n2059: 0.000000%\n2060: 0.000000%\n2061: 0.000000%\n2062: 1.000000%\n2063: 3.333333%\n2064: 0.000000%\n2065: 0.000000%\n2066: 0.000000%\n2067: 0.000000%\n2068: 0.000000%\n2069: 0.000000%\n2070: 0.000000%\n2071: 0.000000%\n2072: 0.000000%\n2073: 0.000000%\n2074: 0.000000%\n2075: 0.000000%\n2076: 1.111111%\n2077: 0.000000%\n2078: 0.000000%\n2079: 0.000000%\n2080: 0.000000%\n2081: 0.000000%\n2082: 0.000000%\n2083: 0.000000%\n2084: 0.000000%\n2085: 0.000000%\n2086: 0.000000%\n2087: 0.000000%\n2088: 1.666667%\n2089: 0.000000%\n2090: 0.000000%\n2091: 0.000000%\n2092: 0.000000%\n"}, {"output_type": "stream", "name": "stdout", "text": "2093: 0.000000%\n2094: 0.000000%\n2095: 0.000000%\n2096: 0.000000%\n2097: 0.000000%\n2098: 0.000000%\n2099: 0.000000%\n2100: 0.000000%\n2101: 0.000000%\n2102: 0.000000%\n2103: 0.000000%\n2104: 0.000000%\n2105: 0.000000%\n2106: 0.000000%\n2107: 0.000000%\n2108: 0.000000%\n2109: 0.000000%\n2110: 0.000000%\n2111: 0.000000%\n2112: 0.000000%\n2113: 0.000000%\n2114: 3.333333%\n2115: 0.000000%\n2116: 0.000000%\n2117: 0.000000%\n2118: 0.000000%\n2119: 1.111111%\n2120: 0.000000%\n2121: 0.000000%\n2122: 0.833333%\n2123: 0.000000%\n2124: 0.000000%\n2125: 0.000000%\n"}, {"ename": "KeyboardInterrupt", "evalue": "", "traceback": ["\u001b[0;31m\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", "\u001b[0;32m<ipython-input-140-7260146ac036>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_target\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}: {:3.6f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/home/nbuser/anaconda2_20/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "], "output_type": "error"}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.13", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}